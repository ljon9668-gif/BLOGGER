{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Blog Migration Tool - Google Colab Version\n",
    "\n",
    "Automated blog content migration with AI-powered rewriting using Gemini AI.\n",
    "\n",
    "## Features:\n",
    "- Extract content from any blog URL (RSS or web scraping)\n",
    "- Rewrite content with AI to avoid plagiarism\n",
    "- Publish automatically to Blogger\n",
    "- Schedule posts over time\n",
    "\n",
    "## Setup:\n",
    "1. Run all cells in order\n",
    "2. Configure your API keys when prompted\n",
    "3. Add source blog URLs\n",
    "4. Process and publish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q supabase beautifulsoup4 feedparser google-api-python-client google-genai pandas requests trafilatura python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Enter your API keys here\n",
    "os.environ['SUPABASE_URL'] = 'https://cxcsfgrsuqlmxcbnkbik.supabase.co'\n",
    "os.environ['SUPABASE_ANON_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImN4Y3NmZ3JzdXFsbXhjYm5rYmlrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTkzMzI2NjQsImV4cCI6MjA3NDkwODY2NH0.mXNM_nI2Rv_29EL3pkaWNemceqNtszHxAH7WWOs94CQ'\n",
    "\n",
    "# Your API keys\n",
    "GEMINI_API_KEY = 'AIzaSyBAqMxp0-Uf9asMQeDCV8uafPYafHXWLI8'\n",
    "BLOGGER_API_KEY = 'AIzaSyBwwg3SyVN9xslSubGlx5kzJMjgHtZibw8'\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
    "os.environ['BLOGGER_API_KEY'] = BLOGGER_API_KEY\n",
    "\n",
    "print('‚úÖ Configuration loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Module\n",
    "from supabase import create_client, Client\n",
    "\n",
    "class Database:\n",
    "    def __init__(self):\n",
    "        supabase_url = os.getenv('SUPABASE_URL')\n",
    "        supabase_key = os.getenv('SUPABASE_ANON_KEY')\n",
    "        self.client: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "    def add_source(self, url: str, name: str) -> bool:\n",
    "        try:\n",
    "            self.client.table('sources').insert({'url': url, 'name': name}).execute()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def get_all_sources(self) -> List[Dict]:\n",
    "        response = self.client.table('sources').select('*').order('created_at', desc=True).execute()\n",
    "        return response.data\n",
    "\n",
    "    def add_post(self, source_id: str, title: str, content: str, source_url: str,\n",
    "                 images: List = None, tags: List = None) -> str:\n",
    "        response = self.client.table('posts').insert({\n",
    "            'source_id': source_id,\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'source_url': source_url,\n",
    "            'images': images or [],\n",
    "            'tags': tags or [],\n",
    "            'status': 'extracted'\n",
    "        }).execute()\n",
    "        return response.data[0]['id']\n",
    "\n",
    "    def is_duplicate(self, title: str, source_url: str) -> bool:\n",
    "        response = self.client.table('posts').select('id').or_(f'title.eq.{title},source_url.eq.{source_url}').limit(1).execute()\n",
    "        return len(response.data) > 0\n",
    "\n",
    "    def get_posts_by_status(self, status: str, limit: int = 100) -> List[Dict]:\n",
    "        response = self.client.table('posts').select('*').eq('status', status).order('created_at', desc=True).limit(limit).execute()\n",
    "        return response.data\n",
    "\n",
    "    def update_post_rewritten(self, post_id: str, rewritten_title: str, rewritten_content: str,\n",
    "                             meta_description: str = None, suggested_tags: List = None):\n",
    "        self.client.table('posts').update({\n",
    "            'rewritten_title': rewritten_title,\n",
    "            'rewritten_content': rewritten_content,\n",
    "            'meta_description': meta_description,\n",
    "            'suggested_tags': suggested_tags or [],\n",
    "            'status': 'rewritten'\n",
    "        }).eq('id', post_id).execute()\n",
    "\n",
    "    def update_post_published(self, post_id: str, published_url: str):\n",
    "        self.client.table('posts').update({\n",
    "            'published_url': published_url,\n",
    "            'status': 'published'\n",
    "        }).eq('id', post_id).execute()\n",
    "\n",
    "    def get_statistics(self) -> Dict:\n",
    "        sources_count = len(self.client.table('sources').select('id').execute().data)\n",
    "        extracted_count = len(self.client.table('posts').select('id').eq('status', 'extracted').execute().data)\n",
    "        published_count = len(self.client.table('posts').select('id').eq('status', 'published').execute().data)\n",
    "        pending_count = len(self.client.table('posts').select('id').in_('status', ['extracted', 'rewritten']).execute().data)\n",
    "        return {\n",
    "            'total_sources': sources_count,\n",
    "            'total_extracted': extracted_count,\n",
    "            'total_published': published_count,\n",
    "            'total_pending': pending_count\n",
    "        }\n",
    "\n",
    "print('‚úÖ Database module loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Extractor Module\n",
    "import feedparser\n",
    "import trafilatura\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "class ContentExtractor:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "\n",
    "    def extract_from_url(self, url: str, max_posts: int = 10) -> List[Dict]:\n",
    "        posts = self._extract_from_rss(url, max_posts)\n",
    "        if not posts:\n",
    "            posts = self._extract_from_webpage(url, max_posts)\n",
    "        return posts\n",
    "\n",
    "    def _extract_from_rss(self, url: str, max_posts: int) -> List[Dict]:\n",
    "        try:\n",
    "            feed = feedparser.parse(url)\n",
    "            if not feed.entries:\n",
    "                return []\n",
    "            \n",
    "            posts = []\n",
    "            for entry in feed.entries[:max_posts]:\n",
    "                content = ''\n",
    "                if 'content' in entry:\n",
    "                    content = entry.content[0].get('value', '')\n",
    "                elif 'summary' in entry:\n",
    "                    content = entry.get('summary', '')\n",
    "                \n",
    "                posts.append({\n",
    "                    'title': entry.get('title', 'Untitled'),\n",
    "                    'url': entry.get('link', url),\n",
    "                    'content': self._clean_html(content),\n",
    "                    'images': [],\n",
    "                    'tags': [tag.term for tag in entry.get('tags', [])]\n",
    "                })\n",
    "            return posts\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _extract_from_webpage(self, url: str, max_posts: int) -> List[Dict]:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_links = self._find_article_links(soup, url)\n",
    "            \n",
    "            posts = []\n",
    "            for link in article_links[:max_posts]:\n",
    "                post = self._scrape_single_article(link)\n",
    "                if post:\n",
    "                    posts.append(post)\n",
    "            return posts\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _scrape_single_article(self, url: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            content = trafilatura.extract(downloaded)\n",
    "            if not content:\n",
    "                return None\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            return {\n",
    "                'title': self._extract_title(soup),\n",
    "                'url': url,\n",
    "                'content': content,\n",
    "                'images': [],\n",
    "                'tags': []\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _find_article_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        links = []\n",
    "        for selector in ['article a[href]', '.post a[href]', 'h2 a[href]', 'h3 a[href]']:\n",
    "            for elem in soup.select(selector):\n",
    "                href = elem.get('href')\n",
    "                if href:\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "                        links.append(full_url)\n",
    "        return list(set(links))\n",
    "\n",
    "    def _extract_title(self, soup: BeautifulSoup) -> str:\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            return h1.get_text(strip=True)\n",
    "        title_tag = soup.find('title')\n",
    "        return title_tag.get_text(strip=True) if title_tag else 'Untitled Post'\n",
    "\n",
    "    def _clean_html(self, html: str) -> str:\n",
    "        if not html:\n",
    "            return ''\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "        return soup.get_text(separator=' ').strip()\n",
    "\n",
    "print('‚úÖ Content Extractor module loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Rewriter Module\n",
    "from google import genai\n",
    "\n",
    "class AIRewriter:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.model = 'gemini-2.0-flash-exp'\n",
    "\n",
    "    def rewrite_post(self, title: str, content: str) -> Dict:\n",
    "        prompt = f\"\"\"Rewrite this blog post completely to avoid plagiarism while preserving the core message.\n",
    "\n",
    "Original Title: {title}\n",
    "Original Content: {content}\n",
    "\n",
    "Output Format:\n",
    "REWRITTEN_TITLE:\n",
    "[title here]\n",
    "\n",
    "REWRITTEN_CONTENT:\n",
    "[content here]\n",
    "\n",
    "META_DESCRIPTION:\n",
    "[150-160 character description]\n",
    "\n",
    "TAGS:\n",
    "[comma-separated tags]\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.client.models.generate_content(model=self.model, contents=prompt)\n",
    "        return self._parse_response(response.text)\n",
    "\n",
    "    def _parse_response(self, text: str) -> Dict:\n",
    "        result = {'title': '', 'content': '', 'meta_description': '', 'tags': []}\n",
    "        \n",
    "        if 'REWRITTEN_TITLE:' in text:\n",
    "            result['title'] = text.split('REWRITTEN_TITLE:')[1].split('REWRITTEN_CONTENT:')[0].strip()\n",
    "        if 'REWRITTEN_CONTENT:' in text:\n",
    "            result['content'] = text.split('REWRITTEN_CONTENT:')[1].split('META_DESCRIPTION:')[0].strip()\n",
    "        if 'META_DESCRIPTION:' in text:\n",
    "            result['meta_description'] = text.split('META_DESCRIPTION:')[1].split('TAGS:')[0].strip()\n",
    "        if 'TAGS:' in text:\n",
    "            tags_text = text.split('TAGS:')[1].strip()\n",
    "            result['tags'] = [tag.strip() for tag in tags_text.split(',')]\n",
    "        \n",
    "        return result\n",
    "\n",
    "print('‚úÖ AI Rewriter module loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blogger Publisher Module\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "class BloggerPublisher:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.service = build('blogger', 'v3', developerKey=api_key)\n",
    "\n",
    "    def publish_post(self, blog_id: str, title: str, content: str, labels: List = None) -> str:\n",
    "        post_body = {\n",
    "            'kind': 'blogger#post',\n",
    "            'blog': {'id': blog_id},\n",
    "            'title': title,\n",
    "            'content': content\n",
    "        }\n",
    "        if labels:\n",
    "            post_body['labels'] = labels\n",
    "        \n",
    "        response = self.service.posts().insert(blogId=blog_id, body=post_body).execute()\n",
    "        return response.get('url', '')\n",
    "\n",
    "print('‚úÖ Blogger Publisher module loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all modules\n",
    "db = Database()\n",
    "extractor = ContentExtractor()\n",
    "rewriter = AIRewriter(GEMINI_API_KEY)\n",
    "publisher = BloggerPublisher(BLOGGER_API_KEY)\n",
    "\n",
    "print('üöÄ All modules initialized successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Dashboard\n",
    "stats = db.get_statistics()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<h2>üìä Dashboard</h2>\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "    <div style=\"padding: 20px; background: #f0f0f0; border-radius: 8px;\">\n",
    "        <h3>{stats['total_sources']}</h3>\n",
    "        <p>Total Sources</p>\n",
    "    </div>\n",
    "    <div style=\"padding: 20px; background: #e3f2fd; border-radius: 8px;\">\n",
    "        <h3>{stats['total_extracted']}</h3>\n",
    "        <p>Extracted Posts</p>\n",
    "    </div>\n",
    "    <div style=\"padding: 20px; background: #c8e6c9; border-radius: 8px;\">\n",
    "        <h3>{stats['total_published']}</h3>\n",
    "        <p>Published Posts</p>\n",
    "    </div>\n",
    "    <div style=\"padding: 20px; background: #fff9c4; border-radius: 8px;\">\n",
    "        <h3>{stats['total_pending']}</h3>\n",
    "        <p>Pending Posts</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Source Blog\n",
    "source_url = input('Enter blog homepage URL: ')\n",
    "source_name = input('Enter a name for this source (optional): ') or source_url\n",
    "\n",
    "if db.add_source(source_url, source_name):\n",
    "    print(f'‚úÖ Added source: {source_name}')\n",
    "else:\n",
    "    print('‚ùå Source already exists or invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Content from Sources\n",
    "sources = db.get_all_sources()\n",
    "\n",
    "if not sources:\n",
    "    print('No sources available. Add sources first.')\n",
    "else:\n",
    "    print('Available sources:')\n",
    "    for i, source in enumerate(sources):\n",
    "        print(f\"{i+1}. {source['name']} - {source['url']}\")\n",
    "    \n",
    "    source_idx = int(input('Select source number: ')) - 1\n",
    "    max_posts = int(input('Maximum posts to extract (default 10): ') or '10')\n",
    "    \n",
    "    selected_source = sources[source_idx]\n",
    "    \n",
    "    print(f\"\\nüîç Extracting content from {selected_source['name']}...\")\n",
    "    posts = extractor.extract_from_url(selected_source['url'], max_posts)\n",
    "    \n",
    "    added = 0\n",
    "    for post in posts:\n",
    "        if not db.is_duplicate(post['title'], post['url']):\n",
    "            db.add_post(\n",
    "                source_id=selected_source['id'],\n",
    "                title=post['title'],\n",
    "                content=post['content'],\n",
    "                source_url=post['url'],\n",
    "                images=post.get('images', []),\n",
    "                tags=post.get('tags', [])\n",
    "            )\n",
    "            added += 1\n",
    "    \n",
    "    print(f'\\n‚úÖ Extracted {added} new posts!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite Posts with AI\n",
    "posts_to_rewrite = db.get_posts_by_status('extracted')\n",
    "\n",
    "if not posts_to_rewrite:\n",
    "    print('No posts to rewrite. Extract content first.')\n",
    "else:\n",
    "    print(f'Found {len(posts_to_rewrite)} posts to rewrite')\n",
    "    process_count = int(input(f'How many posts to process (max {len(posts_to_rewrite)}): '))\n",
    "    \n",
    "    for i, post in enumerate(posts_to_rewrite[:process_count]):\n",
    "        print(f\"\\n[{i+1}/{process_count}] Rewriting: {post['title'][:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            rewritten = rewriter.rewrite_post(post['title'], post['content'])\n",
    "            \n",
    "            db.update_post_rewritten(\n",
    "                post_id=post['id'],\n",
    "                rewritten_title=rewritten['title'],\n",
    "                rewritten_content=rewritten['content'],\n",
    "                meta_description=rewritten.get('meta_description'),\n",
    "                suggested_tags=rewritten.get('tags', [])\n",
    "            )\n",
    "            \n",
    "            print('‚úÖ Rewritten successfully')\n",
    "        except Exception as e:\n",
    "            print(f'‚ùå Error: {str(e)}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Processed {process_count} posts!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish to Blogger\n",
    "blog_id = input('Enter your Blogger Blog ID: ')\n",
    "posts_to_publish = db.get_posts_by_status('rewritten')\n",
    "\n",
    "if not posts_to_publish:\n",
    "    print('No posts ready to publish. Rewrite posts first.')\n",
    "else:\n",
    "    print(f'Found {len(posts_to_publish)} posts ready to publish')\n",
    "    publish_count = int(input(f'How many posts to publish: '))\n",
    "    \n",
    "    for i, post in enumerate(posts_to_publish[:publish_count]):\n",
    "        print(f\"\\n[{i+1}/{publish_count}] Publishing: {post['rewritten_title'][:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            published_url = publisher.publish_post(\n",
    "                blog_id=blog_id,\n",
    "                title=post['rewritten_title'],\n",
    "                content=post['rewritten_content'],\n",
    "                labels=post.get('suggested_tags', [])\n",
    "            )\n",
    "            \n",
    "            db.update_post_published(post_id=post['id'], published_url=published_url)\n",
    "            \n",
    "            print(f'‚úÖ Published: {published_url}')\n",
    "        except Exception as e:\n",
    "            print(f'‚ùå Error: {str(e)}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Published {publish_count} posts!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Complete!\n",
    "\n",
    "You have successfully migrated blog content with AI rewriting!\n",
    "\n",
    "### Next Steps:\n",
    "- Add more source blogs\n",
    "- Extract more content\n",
    "- Process and publish in batches\n",
    "- Monitor your published posts on Blogger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
